{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading\n",
    "## Saisriram Gurajala\n",
    "## 10/27/2024 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook loads in the geojson data from the USGS Wildland Fire dataset, and filters the data by year and proximity to my assigned city: Dearborn, Michigan. This notebook references code from Dr. David W. McDonald's source notebook, `wildfire_geo_proximity_example`, which is housed in the `src/notebooks` directory with no modification. It additionally calls the AQI API from the EPA, and references code from Dr. David W. McDonald's source code notebook: `epa_air_quality_history_example.ipynb`. This notebook is housed in the `src/notebooks` directory with no modification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading GeoJSON Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An executable version of this script is found in `libs/load_and_filter_json.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries and Read in Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we set a base path for the directory this is housed in, as we will need to reference different subdirectories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from libs.helpers import *\n",
    "from libs.api_key_store import Api_Key_Store\n",
    "import os\n",
    "import time\n",
    "import logging \n",
    "import json\n",
    "import multiprocessing\n",
    "import geojson\n",
    "import requests\n",
    "import re\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "os.chdir(os.getenv(\"DATA512_BASE_FILE_PATH\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell sets up a logger to track the parsing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO, \n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "                    filename=f\"./logs/geojson_loading.log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This assigns the filename for the data we are going to read in. It also assigns a filename for the output filename, and sets coordinates for Dearborn, Michigan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FILE = \"data/USGS_Wildland_Fire_Combined_Dataset.json\"\n",
    "OUT_FILE = \"data/USGS_Wildland_Fire_Combined_Dataset_filtered.json\"\n",
    "\n",
    "#coords of dearborn MI\n",
    "coords = [42.322262, -83.176315]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading in and filtering JSON Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we process a given feature. We filter based on the fire year and don't record fires from the wrong years. We additionally filter based on the distance. I leverage the `shortest_distance_from_place_to_fire_perimeter` sourced from the `wildfire_geo_proximity_example.ipynb`. Given we are told the reported fire dates, such as specific months and years, are unreliable we focus on the reported fire year.\n",
    "\n",
    "Inputs to this function are: \n",
    "- feature (dict): A dictionary representing a wildfire polygon\n",
    "- coords (tuple) : Default set of coordinates representing where the distance should be calculated from\n",
    "\n",
    "Outputs of this function are: \n",
    "- feature (dict or None): The function returns the original feature if it meets all filtering criteria, with an added Distance_to_DearbornMI attribute. Otherwise, it returns None.\n",
    "\n",
    "This function operates through the following steps:\n",
    "1. Assign ring_data based on rings or curveRings in geometry. If neither exists, set feature to None.\n",
    "2. Check if Fire_Year is between 1961 and 2021.\n",
    "3. If the date criteria are met, calculate the distance from coords to ring_data. If itâ€™s â‰¤ 1800, add Distance_to_DearbornMI to attributes and include the feature; otherwise, set feature to None.\n",
    "4. Log errors and set feature to None if an exception occurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_filter_year_geojson(feature, coords = coords):\n",
    "    \"\"\"\n",
    "    Filter function that operates on features. \n",
    "    \"\"\"\n",
    "    #assign ring data\n",
    "    if 'rings' in feature['geometry']:\n",
    "        ring_data = feature['geometry']['rings'][0]\n",
    "    elif 'curveRings' in feature['geometry']:\n",
    "        ring_data = feature['geometry']['curveRings'][0]\n",
    "    else:\n",
    "        feature = None\n",
    "    try:\n",
    "        #check fire year\n",
    "        if feature['attributes']['Fire_Year'] >= 1961 and feature['attributes']['Fire_Year'] < 2022:\n",
    "            distance = shortest_distance_from_place_to_fire_perimeter(coords, ring_data)\n",
    "            #filter on distance\n",
    "            if distance[0] <= 1800:\n",
    "                feature['attributes']['Distance_to_DearbornMI'] = distance[0]\n",
    "                logging.info(f\"Fire {feature['attributes']['Listed_Fire_Names']} is included!.\")\n",
    "        #otherwise set feature to be none\n",
    "            else:\n",
    "                feature = None\n",
    "        else:\n",
    "            feature = None\n",
    "    except Exception as e:\n",
    "        logging.info(f\"Error for Fire {feature['attributes']['Listed_Fire_Names']}:{e}\")\n",
    "        feature = None\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main function calls the constituent functions outlined above. The file is read in as a geojson object, and logging tracks these steps. We loop through the features from the geojson data and load and filter them. We set up some logging to report out every 10000 processed features.\n",
    "\n",
    "The main function has no inputs or outputs, and leverages constants set above.\n",
    "\n",
    "This function operates through the following steps:\n",
    "\n",
    "1. Open DATA_FILE and load GeoJSON data into gj_data. Log that data has been read.\n",
    "2. Extract features from gj_data, initialize filtered_features as an empty list, and set feature_number to 0.\n",
    "3. Use load_and_filter_year_geojson to filter each feature. If a feature passes all filters, append its attributes to filtered_features. Log every 10,000 features processed.\n",
    "4. Write filtered_features to OUT_FILE in JSON format and log the total number written.\n",
    "5. Run main and log the time taken to run the script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    #Open file \n",
    "    geojson_file = open(DATA_FILE,\"r\")\n",
    "    gj_data = geojson.load(geojson_file)\n",
    "    #read in data\n",
    "    logging.info(\"Data Read in!\")\n",
    "    #\n",
    "    features = gj_data['features']\n",
    "    filtered_features= []\n",
    "    feature_number = 0\n",
    "    for feature in features:\n",
    "        feature = load_and_filter_year_geojson(feature)\n",
    "        feature_number += 1\n",
    "        if feature:\n",
    "            filtered_features.append(feature['attributes'])\n",
    "        if feature_number % 10000 == 0:\n",
    "            logging.info(f\"Processed and filtered {feature_number} number of features sofar.\")\n",
    "    with open(OUT_FILE, \"w\") as output_file:\n",
    "        json.dump(filtered_features, output_file)\n",
    "    logging.info(f\"A total of {len(filtered_features)} written.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    start = datetime.now()\n",
    "    main() \n",
    "    logging.info(f\"Script has taken {datetime.now() - start} seconds to execute!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AQI API Call"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An executable version of this script is found in `libs/api_call_AQI.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The section below is largely adapted from Dr. David McDonald's `epa_air_quality_history_example.ipynb` notebook. This notebook is available in an unaltered form in the `notebooks` subdirectory.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we set up required constants and logging. We also specify an output file.\n",
    "We set up AQS API for air quality data by configuring logging and specifying where to save the output as AQI_Dearborn_Michigan.csv. We initialize an Api_Key_Store for secure API key management and defines the base API URL along with various endpoints for making requests. To keep within the request limit, it includes a throttle mechanism and provides a template for necessary parameters, including pollutant codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO, \n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "                    filename=f\"./logs/api_call.log\")\n",
    "\n",
    "OUT_FILE = \"data/AQI_Dearborn_Michigan.csv\"\n",
    "\n",
    "#########\n",
    "#\n",
    "#    CONSTANTS\n",
    "#\n",
    "\n",
    "#   API Key store to not expose api key endpoint\n",
    "api_key_store = Api_Key_Store()\n",
    "\n",
    "#\n",
    "#    This is the root of all AQS API URLs\n",
    "#\n",
    "API_REQUEST_URL = 'https://aqs.epa.gov/data/api'\n",
    "\n",
    "#\n",
    "#    These are some of the 'actions' we can ask the API to take or requests that we can make of the API\n",
    "#\n",
    "#    Sign-up request - generally only performed once - unless you lose your key\n",
    "API_ACTION_SIGNUP = '/signup?email={email}'\n",
    "#\n",
    "#    List actions provide information on API parameter values that are required by some other actions/requests\n",
    "API_ACTION_LIST_CLASSES = '/list/classes?email={email}&key={key}'\n",
    "API_ACTION_LIST_PARAMS = '/list/parametersByClass?email={email}&key={key}&pc={pclass}'\n",
    "API_ACTION_LIST_SITES = '/list/sitesByCounty?email={email}&key={key}&state={state}&county={county}'\n",
    "#\n",
    "#    Monitor actions are requests for monitoring stations that meet specific criteria\n",
    "API_ACTION_MONITORS_COUNTY = '/monitors/byCounty?email={email}&key={key}&param={param}&bdate={begin_date}&edate={end_date}&state={state}&county={county}'\n",
    "API_ACTION_MONITORS_BOX = '/monitors/byBox?email={email}&key={key}&param={param}&bdate={begin_date}&edate={end_date}&minlat={minlat}&maxlat={maxlat}&minlon={minlon}&maxlon={maxlon}'\n",
    "#\n",
    "#    Summary actions are requests for summary data. These are for daily summaries\n",
    "API_ACTION_DAILY_SUMMARY_COUNTY = '/dailyData/byCounty?email={email}&key={key}&param={param}&bdate={begin_date}&edate={end_date}&state={state}&county={county}'\n",
    "API_ACTION_DAILY_SUMMARY_BOX = '/dailyData/byBox?email={email}&key={key}&param={param}&bdate={begin_date}&edate={end_date}&minlat={minlat}&maxlat={maxlat}&minlon={minlon}&maxlon={maxlon}'\n",
    "#\n",
    "#    It is always nice to be respectful of a free data resource.\n",
    "#    We're going to observe a 100 requests per minute limit - which is fairly nice\n",
    "API_LATENCY_ASSUMED = 0.002       # Assuming roughly 2ms latency on the API and network\n",
    "API_THROTTLE_WAIT = (1.0/100.0)-API_LATENCY_ASSUMED\n",
    "#\n",
    "#\n",
    "#    This is a template that covers most of the parameters for the actions we might take, from the set of actions\n",
    "#    above. In the examples below, most of the time parameters can either be supplied as individual values to a\n",
    "#    function - or they can be set in a copy of the template and passed in with the template.\n",
    "# \n",
    "AQS_REQUEST_TEMPLATE = {\n",
    "    \"email\":      \"\",     \n",
    "    \"key\":        \"\",      \n",
    "    \"state\":      \"26\",     # the two digit state FIPS # as a string\n",
    "    \"county\":     \"163\",     # the three digit county FIPS # as a string\n",
    "    \"begin_date\": \"\",     # the start of a time window in YYYYMMDD format\n",
    "    \"end_date\":   \"\",     # the end of a time window in YYYYMMDD format, begin_date and end_date must be in the same year\n",
    "    \"minlat\":    0.0,\n",
    "    \"maxlat\":    0.0,\n",
    "    \"minlon\":    0.0,\n",
    "    \"maxlon\":    0.0,\n",
    "    \"param\":     \"\",     # a list of comma separated 5 digit codes, max 5 codes requested\n",
    "    \"pclass\":    \"\"      # parameter class is only used by the List calls\n",
    "}\n",
    "\n",
    "AQI_PARAMS_GASEOUS = \"42101,42401,42602,44201\"\n",
    "#\n",
    "#   Particulate AQI pollutants PM10, PM2.5, and Acceptable PM2.5\n",
    "AQI_PARAMS_PARTICULATES = \"81102,88101,88502\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we set up some essential functions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is adapted from Dr.David McDonald's code. It runs an API Call using the api key store to store the username and the key. Additionally, it logs exceptions and it checks whether the response data is present and logs that edge case as well.\n",
    "\n",
    "Inputs to this function are: \n",
    "- param (str): The parameter codes for the data request.\n",
    "- begin_date (str): The start date for the data in YYYYMMDD format.\n",
    "- end_date (str): The end date for the data in YYYYMMDD format.\n",
    "- fips (str): The 5-digit FIPS code (state + county).\n",
    "- endpoint_url (str): The base URL for the API request.\n",
    "- api_key_store (Api_Key_Store): Object to manage API keys.\n",
    "- endpoint_action (str): The specific API action endpoint.\n",
    "- request_template (dict): Template containing API request parameters.\n",
    "- headers (dict): Any additional headers for the request.\n",
    "\n",
    "Outputs of this function are:\n",
    "- json_response (dict or None): The JSON response from the API containing daily summary data, or None if the request fails or returns no data.\n",
    "\n",
    "This function operates through the following steps:\n",
    "\n",
    "1. Fill in the request_template with the provided parameters, prioritizing any given values over the defaults.\n",
    "2. Check that the necessary fields (email, key, param, begin_date, end_date) are present in the request template; raise an exception if any are missing.\n",
    "3. Build the full API request URL using the base URL and endpoint action, substituting in the values from request_template.\n",
    "4. Implement a throttling delay before making the GET request. Log success or failure of the request and handle any exceptions.\n",
    "5. If the response contains no data, log this and return None; otherwise, return the JSON response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def request_daily_summary(param=None,\n",
    "                           begin_date=None, \n",
    "                           end_date=None, \n",
    "                           fips=None,\n",
    "                           endpoint_url = API_REQUEST_URL, \n",
    "                           api_key_store=api_key_store,\n",
    "                           endpoint_action = API_ACTION_DAILY_SUMMARY_COUNTY, \n",
    "                           request_template = AQS_REQUEST_TEMPLATE,\n",
    "                           headers = None):\n",
    "    \n",
    "    #  This prioritizes the info from the call parameters - not what's already in the template\n",
    "    if api_key_store:\n",
    "        request_template['email'] = api_key_store.username\n",
    "        request_template['key'] = api_key_store.key\n",
    "    if param:\n",
    "        request_template['param'] = param\n",
    "    if begin_date:\n",
    "        request_template['begin_date'] = begin_date\n",
    "    if end_date:\n",
    "        request_template['end_date'] = end_date\n",
    "    if fips and len(fips)==5:\n",
    "        request_template['state'] = fips[:2]\n",
    "        request_template['county'] = fips[2:]            \n",
    "\n",
    "    # Make sure there are values that allow us to make a call - these are always required\n",
    "    if not request_template['email']:\n",
    "        raise Exception(\"Must supply an email address to call 'request_daily_summary()'\")\n",
    "    if not request_template['key']: \n",
    "        raise Exception(\"Must supply a key to call 'request_daily_summary()'\")\n",
    "    if not request_template['param']: \n",
    "        raise Exception(\"Must supply param values to call 'request_daily_summary()'\")\n",
    "    if not request_template['begin_date']: \n",
    "        raise Exception(\"Must supply a begin_date to call 'request_daily_summary()'\")\n",
    "    if not request_template['end_date']: \n",
    "        raise Exception(\"Must supply an end_date to call 'request_daily_summary()'\")\n",
    "    # Note we're not validating FIPS fields because not all of the annual summary actions require the FIPS numbers\n",
    "    # compose the request\n",
    "    request_url = endpoint_url+endpoint_action.format(**request_template)\n",
    "    # make the request\n",
    "    try:\n",
    "        # Wait first, to make sure we don't exceed a rate limit in the situation where an exception occurs\n",
    "        # during the request processing - throttling is always a good practice with a free data source\n",
    "        if API_THROTTLE_WAIT > 0.0:\n",
    "            time.sleep(API_THROTTLE_WAIT)\n",
    "        response = requests.get(request_url, headers=headers)\n",
    "        json_response = response.json()\n",
    "        logging.info(f\"API Pull for {begin_date} to {end_date} succeeded for param {param}.\")\n",
    "    except Exception as e:\n",
    "        logging.info(f\"API Pull for {begin_date} to {end_date} for param {param} failed due to reason {e}.\")\n",
    "        json_response = None\n",
    "    \n",
    "    if json_response['Data'] == []:\n",
    "        logging.info(f\"No data available for API Call of {begin_date} to {end_date} for param {param}.\")\n",
    "        json_response = None\n",
    "    return json_response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function takes in gas and particulate pollutant AQI data and consolidates it into a single record per day. A given parameter may be measured at multiple different stations per day. Therefore, we need a way to conslidate AQI per day for a given parameter p. I chose to calculate this with the following formula, which averages AQI per parameter per day. \n",
    "\n",
    "$$AQI_{day_p} = \\frac{1}{n}\\sum_{x=0}^{n} AQI^{x}_{day_p}$$\n",
    "\n",
    "where n is the total number of measurements for a given parameter on a given day.\n",
    "\n",
    "We then produce a single AQI measurement per day by taking the maximum average AQI measurement per parameter.\n",
    "\n",
    "$$AQI_{day} = max([AQI_{day_p}]^{params}_p)$$\n",
    "\n",
    "where p is a parameter in the set of gaseous and particulate pollutant parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following functions consolidate the AQI data for gaseous and particulate pollutants according to the above formulas. We additionally define the fire season with this function as being between May 1st and October 31st. \n",
    "\n",
    "Inputs to this function are: \n",
    "- res_gas (dict): Response data for gaseous pollutants.\n",
    "- res_particulates (dict): Response data for particulate pollutants.\n",
    "- year (int): The year for which data is being consolidated.\n",
    "\n",
    "Outputs to this function are: \n",
    "- response_df (DataFrame): A Pandas DataFrame containing consolidated AQI data per date, or None if consolidation fails or no valid data is provided.\n",
    "\n",
    "This function operates through the following steps:\n",
    "1. Verify that either res_gas or res_particulates is provided, and that a valid year is specified. Log messages if checks fail and return None.\n",
    "2. Define lower and upper bounds for the given year (May 1st to October 31st).\n",
    "3. Loop through res_gas['Data'], filtering entries within the date bounds and appending valid data to a list of DataFrames.\n",
    "4. Repeat the filtering and appending process for res_particulates['Data'].\n",
    "5. Concatenate all collected DataFrames, drop any NaN values, and calculate the mean AQI per date and parameter. Then, group by date to take the maximum AQI for each day across params.\n",
    "6. Catch any exceptions during the consolidation process, log the error, and return None if an error occurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def consolidate_results(res_gas = None, \n",
    "                        res_particulates = None,\n",
    "                        year = None):\n",
    "    if not res_gas and not res_particulates:\n",
    "        logging.info(\"Must provide valid data to consolidate!\")\n",
    "        response_df = None\n",
    "        return response_df \n",
    "    if not year:\n",
    "        logging.info(\"Must provide a valid year to consolidate data\")\n",
    "        response_df = None\n",
    "        return response_df\n",
    "    #set upper and lower bounds for years\n",
    "    lower_bound = datetime.datetime.strptime(f\"{year}-05-01\", \"%Y-%m-%d\")\n",
    "    upper_bound = datetime.datetime.strptime(f\"{year}-10-31\", \"%Y-%m-%d\")\n",
    "    response_dataframes = []\n",
    "    #consolidate res for gas\n",
    "    try:\n",
    "        if res_gas: \n",
    "            for response in res_gas['Data']:\n",
    "                res_date = datetime.datetime.strptime(response['date_local'], \"%Y-%m-%d\")\n",
    "                if res_date >= lower_bound and res_date <= upper_bound:\n",
    "                    res_df = pd.DataFrame({\"date\" : [response['date_local']],\n",
    "                                        \"parameter\" : [response['parameter']],\n",
    "                                        \"aqi\" : [response['aqi']]})\n",
    "                    response_dataframes.append(res_df)\n",
    "        #consolidate res for particulates\n",
    "        if res_particulates:\n",
    "            for response in res_particulates['Data']:\n",
    "                res_date = datetime.datetime.strptime(response['date_local'], \"%Y-%m-%d\")\n",
    "                if res_date >= lower_bound and res_date <= upper_bound:\n",
    "                    res_df = pd.DataFrame({\"date\" : [response['date_local']],\n",
    "                                        \"parameter\" : [response['parameter']],\n",
    "                                        \"aqi\" : [response['aqi']]})\n",
    "                    response_dataframes.append(res_df)\n",
    "        #take mean per date and parameter\n",
    "        response_df = pd.concat(response_dataframes).dropna().groupby(['date', 'parameter'], as_index=False)['aqi'].mean()\n",
    "        #take AQI for \n",
    "        response_df = response_df.groupby(['date'], as_index = False)['aqi'].max()\n",
    "    except Exception as e:\n",
    "        logging.info(f\"Data consolidation for year {year} failed with error {e}\")\n",
    "        response_df = None\n",
    "    return response_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main function leverages set constants and runs api calls per year range. \n",
    "It has no inputs or outputs. \n",
    "\n",
    "The function operates through the following steps:\n",
    "1. Create an empty list to store yearly AQI DataFrames.\n",
    "2. Loop through each year in reverse from 2021 to 1961. \n",
    "3. Define start and end dates for the year. \n",
    "4. Request daily summaries of gaseous and particulate AQI data. \n",
    "5. Combine and filter results to create a DataFrame per year.\n",
    "6. Filter out empty DataFrames, concatenate the yearly DataFrames, add a year column, and save the result to CSV. \n",
    "7. Main is executed and execution time is logged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    year_dfs = []\n",
    "    #loop through years\n",
    "    for year in reversed(range(1961, 2022)):\n",
    "        start_date = f\"{year}0101\"\n",
    "        end_date = f\"{year}1231\"\n",
    "        #request daily summary for gas and particulate\n",
    "        res_gas = request_daily_summary(param = AQI_PARAMS_GASEOUS,\n",
    "                                        begin_date=start_date,\n",
    "                                        end_date=end_date,\n",
    "                                        api_key_store=api_key_store)\n",
    "        res_particulates = request_daily_summary(param = AQI_PARAMS_PARTICULATES,\n",
    "                                                begin_date=start_date,\n",
    "                                                end_date=end_date,\n",
    "                                                api_key_store=api_key_store)\n",
    "        #consolidate responses\n",
    "        response_df = consolidate_results(res_gas, \n",
    "                                          res_particulates,\n",
    "                                          year)\n",
    "        year_dfs.append(response_df)\n",
    "    #concatenate dataframes\n",
    "    full_dataframe = pd.concat(year_dfs)\n",
    "    full_dataframe['year'] = pd.to_datetime(full_dataframe['date']).dt.year\n",
    "    full_dataframe.to_csv(OUT_FILE, index = False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    start = datetime.datetime.now()\n",
    "    main()\n",
    "    logging.info(f\"Script has taken {datetime.datetime.now() - start} seconds to execute.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
